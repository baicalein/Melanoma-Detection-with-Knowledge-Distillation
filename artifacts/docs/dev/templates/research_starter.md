# Research Notes and Best Practices

This was mostly generated by copilot, but its really useful structure.- [Research Notes and Best Practices](#research-notes-and-best-practices)
- [Research Notes and Best Practices](#research-notes-and-best-practices)
  - [ðŸŽ¯ Academic Research Workflow](#-academic-research-workflow)
    - [Phase 1: Literature Review and Problem Definition](#phase-1-literature-review-and-problem-definition)
    - [Phase 2: Data Preparation](#phase-2-data-preparation)
    - [Phase 3: Baseline Implementation](#phase-3-baseline-implementation)
    - [Phase 4: Experimentation](#phase-4-experimentation)
    - [Phase 5: Analysis and Writing](#phase-5-analysis-and-writing)
  - [ðŸ“Š Experiment Design Principles](#-experiment-design-principles)
    - [The Scientific Method for ML](#the-scientific-method-for-ml)
    - [Ablation Studies](#ablation-studies)
    - [Hyperparameter Tuning](#hyperparameter-tuning)
  - [ðŸ”¬ Common Research Pitfalls (and How to Avoid Them)](#-common-research-pitfalls-and-how-to-avoid-them)
    - [1. Data Leakage](#1-data-leakage)
    - [2. Overfitting to Validation Set](#2-overfitting-to-validation-set)
    - [3. Cherry-picking Results](#3-cherry-picking-results)
    - [4. Not Using Enough Random Seeds](#4-not-using-enough-random-seeds)
    - [5. Improper Baselines](#5-improper-baselines)
  - [ðŸ“ Documentation Best Practices](#-documentation-best-practices)
    - [Lab Notebook (docs/journal.md)](#lab-notebook-docsjournalmd)
    - [Code Documentation](#code-documentation)
    - [Experiment Tracking](#experiment-tracking)
  - [ðŸŽ“ Tips for Academic Success](#-tips-for-academic-success)
    - [Time Management](#time-management)
    - [Collaboration](#collaboration)
    - [Paper Writing Integration](#paper-writing-integration)
  - [ðŸ” Debugging Deep Learning Models](#-debugging-deep-learning-models)
    - [Model Not Learning](#model-not-learning)
    - [Overfitting](#overfitting)
    - [Underfitting](#underfitting)
  - [ðŸ“š Recommended Reading](#-recommended-reading)
    - [Deep Learning](#deep-learning)
    - [Research Methods](#research-methods)
    - [Python Best Practices](#python-best-practices)
  - [ðŸ’¡ Final Tips](#-final-tips)


This document contains helpful notes and best practices specifically for academic research projects in deep learning.

## ðŸŽ¯ Academic Research Workflow

### Phase 1: Literature Review and Problem Definition

**Before writing any code:**

1. **Read relevant papers thoroughly**
   - Keep a bibliography in `docs/references.bib`
   - Take notes in `docs/literature_notes.md`
   - Identify research gaps

2. **Define your research question**
   - Write it clearly in `docs/research_question.md`
   - Define success criteria
   - Plan experiments that answer the question

3. **Design experiments**
   - Start with baseline models
   - Plan ablation studies
   - Consider computational constraints

### Phase 2: Data Preparation

**Critical for reproducibility:**

1. **Document data sources**
   - Create `data/README.md` with:
     - Source URL or citation
     - Download date
     - License information
     - Preprocessing steps

2. **Version control data splits**
   - Save train/val/test split indices
   - Use fixed random seeds
   - Document split ratios

3. **Validate data quality**
   ```python
   # In notebooks/exploratory/data_validation.ipynb
   - Check for missing values
   - Visualize class distributions
   - Identify outliers
   - Verify data statistics match reported values
   ```

### Phase 3: Baseline Implementation

**Start simple, then iterate:**

1. **Implement the simplest reasonable baseline**
   - Use well-known architectures first
   - Verify it trains properly
   - Get reasonable performance

2. **Sanity checks**
   ```python
   # Test overfitting on small batch
   small_batch = next(iter(train_loader))
   for _ in range(100):
       loss = train_step(model, small_batch)
   # Loss should go to near zero
   ```

3. **Document baseline performance**
   - Save metrics in `results/metrics/baseline_results.json`
   - All future work will be compared to this

### Phase 4: Experimentation

**Systematic approach to trying new ideas:**

1. **Change one thing at a time**
   - Isolate the effect of each modification
   - Enables proper ablation studies
   - Makes debugging easier

2. **Use configuration files**
   ```yaml
   # configs/experiments/experiment_023_attention.yaml
   experiment:
     name: exp_023_attention
     description: "Add attention mechanism to baseline"
     baseline: experiment_001_baseline
     changes:
       - "Added multi-head attention after conv layer 3"
   ```

3. **Track everything**
   - Hyperparameters
   - Random seeds
   - Training time
   - Hardware used
   - Git commit hash

### Phase 5: Analysis and Writing

**Preparing for publication:**

1. **Generate publication-quality figures**
   ```python
   from deep_learning_final_project.visualization.plots import setup_publication_style
   
   setup_publication_style()
   # Create your plots
   plt.savefig('results/figures/training_curves.pdf', dpi=300)
   ```

2. **Create result tables**
   - Save as LaTeX: `results/tables/comparison.tex`
   - Include confidence intervals
   - Report mean Â± std over multiple runs

3. **Statistical significance**
   - Run experiments with different random seeds (3-5 runs)
   - Report mean and standard deviation
   - Consider statistical tests for comparisons

## ðŸ“Š Experiment Design Principles

### The Scientific Method for ML

1. **Hypothesis**: "Adding dropout will reduce overfitting"
2. **Prediction**: "Validation accuracy will improve"
3. **Experiment**: Train with/without dropout
4. **Analysis**: Compare validation curves
5. **Conclusion**: Accept/reject hypothesis

### Ablation Studies

**Essential for understanding what works:**

```yaml
# Example ablation study plan
baseline:
  - Simple CNN
  
add_component_1:
  - Baseline + Batch Normalization
  
add_component_2:
  - Baseline + Dropout
  
full_model:
  - Baseline + BN + Dropout

# This shows individual contribution of each component
```

### Hyperparameter Tuning

**Systematic approach:**

1. **Start with literature values**
   - Use learning rates from similar papers
   - Use standard batch sizes

2. **Tune one hyperparameter at a time**
   - Learning rate first (most important)
   - Then batch size
   - Then architecture-specific parameters

3. **Use proper validation**
   - Never tune on test set
   - Use separate validation set
   - Consider k-fold cross-validation

## ðŸ”¬ Common Research Pitfalls (and How to Avoid Them)

### 1. Data Leakage

**Problem**: Test data influences training
**Solutions**:
- Split data BEFORE any preprocessing
- Never look at test set during development
- Use proper cross-validation

### 2. Overfitting to Validation Set

**Problem**: Selecting models based on validation performance
**Solutions**:
- Use separate validation and test sets
- Limit number of hyperparameter trials
- Report test metrics only once at the end

### 3. Cherry-picking Results

**Problem**: Reporting only the best run
**Solutions**:
- Report mean Â± std over multiple runs
- Use consistent evaluation protocol
- Report negative results too

### 4. Not Using Enough Random Seeds

**Problem**: Results might be due to lucky initialization
**Solutions**:
- Run each experiment with 3-5 different seeds
- Report variance in results
- Be honest about stability

### 5. Improper Baselines

**Problem**: Comparing against weak baselines
**Solutions**:
- Implement strong baselines from literature
- Use properly tuned hyperparameters
- Compare against recent work

## ðŸ“ Documentation Best Practices

### Lab Notebook (docs/journal.md)

Keep a research journal with entries like:

```markdown
## 2024-01-15

### Experiment: exp_023_attention

**Motivation**: 
Hypothesis that attention would help model focus on important features.

**Changes from baseline**:
- Added multi-head attention (4 heads) after conv3
- Increased model parameters by ~20%

**Results**:
- Training converged slower (50 vs 30 epochs)
- Final accuracy: 87.3% (baseline: 85.1%)
- Significant improvement on hard examples

**Next steps**:
- Try different number of attention heads
- Visualize attention weights
- Compare with baseline on per-class basis
```

### Code Documentation

```python
def my_research_function(x: torch.Tensor) -> torch.Tensor:
    """
    Brief description of what this does.
    
    This function implements the technique from [Smith et al., 2023].
    We modified their approach by [describe modification].
    
    Args:
        x: Input tensor of shape (batch_size, channels, height, width)
        
    Returns:
        Processed tensor of same shape as input
        
    References:
        Smith et al., "Paper Title", Conference 2023
    """
    pass
```

### Experiment Tracking

Create `docs/experiments.md`:

```markdown
# Experiment Log

| ID | Date | Description | Config | Val Acc | Test Acc | Notes |
|----|------|-------------|--------|---------|----------|-------|
| 001 | 2024-01-10 | Baseline CNN | baseline.yaml | 85.1% | 84.8% | Good baseline |
| 002 | 2024-01-12 | + Data Aug | exp_002.yaml | 87.2% | 86.9% | Clear improvement |
| 003 | 2024-01-15 | + Attention | exp_003.yaml | 87.3% | - | Still training |
```

## ðŸŽ“ Tips for Academic Success

### Time Management

1. **Set weekly goals**
   - Define concrete deliverables
   - Track progress in `docs/weekly_progress.md`

2. **Regular meetings**
   - Prepare results before advisor meetings
   - Bring plots and figures
   - Have specific questions ready

3. **Manage computational resources**
   - Queue long-running jobs overnight
   - Use cloud credits efficiently
   - Monitor GPU utilization

### Collaboration

1. **Code reviews**
   - Review each other's pull requests
   - Share knowledge about techniques
   - Maintain code quality

2. **Reproducibility**
   - Others should be able to run your code
   - Provide clear README instructions
   - Include example commands

3. **Sharing resources**
   - Document pretrained models
   - Share processed datasets
   - Create reusable utilities

### Paper Writing Integration

1. **Generate figures programmatically**
   ```python
   # scripts/generate_paper_figures.py
   # Regenerate all figures from saved results
   # Ensures consistency if numbers change
   ```

2. **Track experimental decisions**
   - Why did you choose this architecture?
   - Why these hyperparameters?
   - Document in code and paper

3. **Prepare for revision**
   - Keep all experimental results
   - Easy to run new experiments if requested
   - Version control paper drafts

## ðŸ” Debugging Deep Learning Models

### Model Not Learning

**Symptoms**: Loss stays constant or increases

**Checklist**:
- [ ] Check learning rate (try 1e-4, 1e-3, 1e-2)
- [ ] Verify data normalization
- [ ] Check loss function is appropriate
- [ ] Verify gradient flow (no vanishing gradients)
- [ ] Test on small batch (should overfit)
- [ ] Check data augmentation isn't too aggressive

### Overfitting

**Symptoms**: Train accuracy >> Val accuracy

**Solutions**:
- [ ] Add dropout
- [ ] Add weight decay
- [ ] Reduce model capacity
- [ ] Add data augmentation
- [ ] Early stopping
- [ ] Get more training data

### Underfitting

**Symptoms**: Both train and val accuracy low

**Solutions**:
- [ ] Increase model capacity
- [ ] Train longer
- [ ] Reduce regularization
- [ ] Check data preprocessing
- [ ] Verify labels are correct

## ðŸ“š Recommended Reading

### Deep Learning

- "Deep Learning" by Goodfellow, Bengio, and Courville
- "Dive into Deep Learning" (d2l.ai) - Free online book
- CS231n lecture notes (Stanford)

### Research Methods

- "How to Read a Paper" by S. Keshav
- "The Craft of Research" by Booth et al.
- "Ten Simple Rules" series from PLOS Computational Biology

### Python Best Practices

- "Effective Python" by Brett Slatkin
- "Fluent Python" by Luciano Ramalho
- PEP 8 Style Guide

## ðŸ’¡ Final Tips

1. **Embrace failure**: Most experiments won't work
2. **Document everything**: You'll forget why you did things
3. **Ask questions**: No question is too basic
4. **Stay organized**: Future you will be grateful
5. **Take breaks**: Research is a marathon, not a sprint

---

Remember: Good research takes time. Be patient, methodical, and thorough. ðŸŒŸ
